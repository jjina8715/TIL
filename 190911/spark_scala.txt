scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[35] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011,  2016...

scala> val wordCandidateRDD = textRDD.flatMap(_.split(" "))
wordCandidateRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at flatMap at <console>:25

scala> val wordCandidateArray = wordCandidateRDD.collect
wordCandidateArray: Array[String] = Array(201612120944, p001, 201612120944, p003, 201612120944, p...

scala> val wordRDD = wordCandidateRDD.filter(_.matches("^p[0-9]{3}$"))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[48] at filter at <console>:25

scala> wordRDD.collect
res51: Array[String] = Array(p001, p003, p011, p008, p008, p011, p014, p015, p009, p001, p003, p011, p008, p008, ...

scala> val wordAndOnePairRDD = wordRDD.map((_,1))
wordAndOnePairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[49] at map at <console>:25

scala> val wordAndCountRDD = wordAndOnePairRDD.reduceByKey(_+_)
wordAndCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[50] at reduceByKey at <console>:25

scala> val wordAndCountArray = wordAndCountRDD.collect
wordAndCountArray: Array[(String, Int)] = Array((p004,50), (p006,51), (p011,49), (p013,4), (p008,80), (p015,7), (p002,66), (p012,39), (p001,84),
 (p005,43), (p003,54), (p007,71), (p009,80), (p014,10), (p010,58))

scala> wordAndCountArray.foreach(println)
(p004,50)
(p006,51)
(p011,49)
(p013,4)
(p008,80)
(p015,7)
(p002,66)
(p012,39)
(p001,84)
(p005,43)
(p003,54)
(p007,71)
(p009,80)
(p014,10)
(p010,58)
